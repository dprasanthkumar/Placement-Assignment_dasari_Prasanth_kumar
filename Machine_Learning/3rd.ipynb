{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "from nltk.metrics import distance\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dpras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Data preprocessing\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove unwanted characters and convert to lowercase\n",
    "    text = text.lower().replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and perform stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    processed_text = \" \".join(stemmed_tokens)\n",
    "    return processed_text\n",
    "\n",
    "df['processed_text'] = df['headline'] + \" \" + df['short_description']\n",
    "df['processed_text'] = df['processed_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Similarity algorithms\n",
    "def calculate_cosine_similarity(query_vector, features):\n",
    "    similarity_scores = cosine_similarity(query_vector, features).flatten()\n",
    "    return similarity_scores\n",
    "\n",
    "def calculate_jaccard_similarity(query_vector, features):\n",
    "    query_set = set(query_vector.indices)\n",
    "    similarity_scores = []\n",
    "    for feature_vector in features:\n",
    "        feature_set = set(feature_vector.indices)\n",
    "        similarity_scores.append(len(query_set.intersection(feature_set)) / len(query_set.union(feature_set)))\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "def calculate_euclidean_distance(query_vector, features):\n",
    "    # Convert the query vector to a dense array\n",
    "    query_vector = query_vector.toarray()[0]\n",
    "\n",
    "    # Calculate the Euclidean distance between the query vector and each feature vector\n",
    "    distances = np.linalg.norm(query_vector - features, axis=1)\n",
    "\n",
    "    return distances.tolist()\n",
    "\n",
    "\n",
    "def calculate_levenshtein_distance(query_text, features):\n",
    "    similarity_scores = []\n",
    "    for feature_text in features:\n",
    "        similarity_scores.append(distance.edit_distance(query_text, feature_text))\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Model building\n",
    "def find_most_similar_data(query_text, df, top_k=5, batch_size=1000):\n",
    "    query_text = preprocess_text(query_text)\n",
    "    query_vector = vectorizer.transform([query_text])\n",
    "\n",
    "    num_batches = len(df) // batch_size\n",
    "    if len(df) % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    cosine_similarities = []\n",
    "    jaccard_similarities = []\n",
    "    euclidean_distances = []\n",
    "    levenshtein_distances = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_features = features[start_idx:end_idx]\n",
    "\n",
    "        cosine_similarities.extend(calculate_cosine_similarity(query_vector, batch_features))\n",
    "        jaccard_similarities.extend(calculate_jaccard_similarity(query_vector, batch_features))\n",
    "        euclidean_distances.extend(calculate_euclidean_distance(query_vector, batch_features))\n",
    "        levenshtein_distances.extend(calculate_levenshtein_distance(query_text, df['processed_text'][start_idx:end_idx]))\n",
    "\n",
    "    df['cosine_similarity'] = cosine_similarities\n",
    "    df['jaccard_similarity'] = jaccard_similarities\n",
    "    df['euclidean_distance'] = euclidean_distances\n",
    "    df['levenshtein_distance'] = levenshtein_distances\n",
    "\n",
    "    # Rank the data points based on similarity scores\n",
    "    df = df.sort_values(by=['cosine_similarity', 'jaccard_similarity', 'euclidean_distance', 'levenshtein_distance'], ascending=False)\n",
    "\n",
    "    # Return the top-k most similar data points\n",
    "    most_similar_data = df.head(top_k)\n",
    "    return most_similar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 headline  \\\n",
      "144273  AI Day Will Replace Christmas as the Most Impo...   \n",
      "63721   Four Incredible New Advances in Health Technology   \n",
      "123313      Because You Ain't Never Had A Friend Like Him   \n",
      "88965   Trump Supporters Harass Immigration Protesters...   \n",
      "45603   This Is Why Your Coffee Is About To Get More E...   \n",
      "\n",
      "                                        short_description  \n",
      "144273  Some religious people, anti-transhumanists, an...  \n",
      "63721                                                      \n",
      "123313                                                     \n",
      "88965   Trump supporters shouted things like \"If it ai...  \n",
      "45603                                    Say it ain't so!  \n"
     ]
    }
   ],
   "source": [
    "# Step 6: Model\n",
    "query_text = \"Latest technology advancements in AI\"\n",
    "similar_data = find_most_similar_data(query_text, df)\n",
    "print(similar_data[['headline', 'short_description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
